{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"name":"natural-language-processing-with-disaster-tweets.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"code","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-06T21:53:16.277887Z","iopub.execute_input":"2021-07-06T21:53:16.278328Z","iopub.status.idle":"2021-07-06T21:53:16.307880Z","shell.execute_reply.started":"2021-07-06T21:53:16.278291Z","shell.execute_reply":"2021-07-06T21:53:16.306227Z"},"trusted":true,"id":"rgeNtLmm1Taq","executionInfo":{"status":"ok","timestamp":1633531826360,"user_tz":-60,"elapsed":262,"user":{"displayName":"ines souissi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10446406516205922358"}}},"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IJQFfc4-3078"},"source":["# Nouvelle section"]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-07-06T21:53:16.410362Z","iopub.execute_input":"2021-07-06T21:53:16.410782Z","iopub.status.idle":"2021-07-06T21:53:16.481219Z","shell.execute_reply.started":"2021-07-06T21:53:16.410739Z","shell.execute_reply":"2021-07-06T21:53:16.479713Z"},"trusted":true,"id":"mPvujWNU1Tat"},"source":["import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","from sklearn import feature_extraction, linear_model, model_selection, preprocessing\n","##\n","import os\n","import warnings\n","from pathlib import Path\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","from IPython.display import display\n","from pandas.api.types import CategoricalDtype\n","\n","from sklearn.cluster import KMeans\n","from sklearn.decomposition import PCA\n","from sklearn.feature_selection import mutual_info_regression\n","from sklearn.model_selection import KFold, cross_val_score\n","from xgboost import XGBRegressor\n","\n","df_train = pd.read_csv(\"/content/train.csv\")\n","df_test = pd.read_csv(\"/content/test.csv\")\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pzYS7kbS4jyR"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-07-06T21:53:16.527785Z","iopub.execute_input":"2021-07-06T21:53:16.528430Z","iopub.status.idle":"2021-07-06T21:53:16.550917Z","shell.execute_reply.started":"2021-07-06T21:53:16.528379Z","shell.execute_reply":"2021-07-06T21:53:16.549311Z"},"trusted":true,"id":"ieiZ6Ms21Tau"},"source":["df_train.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-07-06T21:53:16.623846Z","iopub.execute_input":"2021-07-06T21:53:16.624354Z","iopub.status.idle":"2021-07-06T21:53:16.630686Z","shell.execute_reply.started":"2021-07-06T21:53:16.624316Z","shell.execute_reply":"2021-07-06T21:53:16.629153Z"},"trusted":true,"id":"sxTx6rks1Tav"},"source":["import sklearn\n","import keras\n","import nltk\n","import pandas as pd\n","import numpy as np\n","import re\n","import codecs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-07-06T21:53:16.766792Z","iopub.execute_input":"2021-07-06T21:53:16.767273Z","iopub.status.idle":"2021-07-06T21:53:16.885403Z","shell.execute_reply.started":"2021-07-06T21:53:16.767235Z","shell.execute_reply":"2021-07-06T21:53:16.883958Z"},"trusted":true,"id":"AtuUq6f71Tav"},"source":["input_file = codecs.open(\"/content/train.csv\", \"r\",encoding='utf-8', errors='replace')\n","output_file = open(\"train_output.csv\", \"w\")\n","\n","def sanitize_characters(raw, clean):    \n","    for line in input_file:\n","        out = line\n","        output_file.write(line)\n","sanitize_characters(input_file, output_file)\n","\n","questions = pd.read_csv(\"/content/train_output.csv\")\n","questions.columns=['id','keyword', 'location', 'text', 'targte']\n","questions.head()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-07-06T21:53:16.891613Z","iopub.execute_input":"2021-07-06T21:53:16.892093Z","iopub.status.idle":"2021-07-06T21:53:17.112955Z","shell.execute_reply.started":"2021-07-06T21:53:16.892049Z","shell.execute_reply":"2021-07-06T21:53:17.111752Z"},"trusted":true,"id":"FoqscFQA1Taw"},"source":["\n","def standardize_text(df, text_field):\n","    df[text_field] = df[text_field].str.replace(r\"http\\S+\", \"\")\n","    df[text_field] = df[text_field].str.replace(r\"http\", \"\")\n","    df[text_field] = df[text_field].str.replace(r\"@\\S+\", \"\")\n","    df[text_field] = df[text_field].str.replace(r\"[^A-Za-z0-9(),!?@\\'\\`\\\"\\_\\n]\", \" \")\n","    df[text_field] = df[text_field].str.replace(r\"@\", \"at\")\n","    df[text_field] = df[text_field].str.lower()\n","    return df\n","\n","df_train = standardize_text(df_train, \"text\")\n","df_train.to_csv(\"clean_data.csv\")\n","df_train.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-07-06T21:53:17.114652Z","iopub.execute_input":"2021-07-06T21:53:17.114947Z","iopub.status.idle":"2021-07-06T21:53:17.133132Z","shell.execute_reply.started":"2021-07-06T21:53:17.114918Z","shell.execute_reply":"2021-07-06T21:53:17.131535Z"},"trusted":true,"id":"8kzqsz4y1Taw"},"source":["df_train.groupby(\"target\").count()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GlT1Gfbm1Tax"},"source":[""]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-07-06T21:53:17.135410Z","iopub.execute_input":"2021-07-06T21:53:17.135766Z","iopub.status.idle":"2021-07-06T21:53:17.230662Z","shell.execute_reply.started":"2021-07-06T21:53:17.135732Z","shell.execute_reply":"2021-07-06T21:53:17.229122Z"},"trusted":true,"id":"41q02yv71Tax"},"source":["from nltk.tokenize import RegexpTokenizer\n","\n","tokenizer = RegexpTokenizer(r'\\w+')\n","\n","df_train[\"tokens\"] = df_train[\"text\"].apply(tokenizer.tokenize)\n","df_train.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-07-06T21:53:17.233224Z","iopub.execute_input":"2021-07-06T21:53:17.233758Z","iopub.status.idle":"2021-07-06T21:53:17.271317Z","shell.execute_reply.started":"2021-07-06T21:53:17.233703Z","shell.execute_reply":"2021-07-06T21:53:17.269774Z"},"trusted":true,"id":"NmKhbNKx1Tax"},"source":["#Inspecting dataset \n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","#from keras.utils import to_categorical\n","\n","all_words = [word for tokens in df_train[\"tokens\"] for word in tokens]\n","sentence_lengths = [len(tokens) for tokens in df_train[\"tokens\"]]\n","VOCAB = sorted(list(set(all_words)))\n","print(\"%s words total, with a vocabulary size of %s\" % (len(all_words), len(VOCAB)))\n","print(\"Max sentence length is %s\" % max(sentence_lengths))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-07-06T21:53:17.331807Z","iopub.execute_input":"2021-07-06T21:53:17.332223Z","iopub.status.idle":"2021-07-06T21:53:17.586584Z","shell.execute_reply.started":"2021-07-06T21:53:17.332186Z","shell.execute_reply":"2021-07-06T21:53:17.585414Z"},"trusted":true,"id":"rAw5oPsL1Tay"},"source":["\n","import matplotlib.pyplot as plt\n","\n","fig = plt.figure(figsize=(10, 10)) \n","plt.xlabel('Sentence length')\n","plt.ylabel('Number of sentences')\n","plt.hist(sentence_lengths)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-07-06T21:53:17.588182Z","iopub.execute_input":"2021-07-06T21:53:17.588479Z","iopub.status.idle":"2021-07-06T21:53:17.854421Z","shell.execute_reply.started":"2021-07-06T21:53:17.588450Z","shell.execute_reply":"2021-07-06T21:53:17.852959Z"},"trusted":true,"id":"pCmkcb9F1Tay"},"source":["\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","\n","def cv(data):\n","    count_vectorizer = CountVectorizer()\n","\n","    emb = count_vectorizer.fit_transform(data)\n","\n","    return emb, count_vectorizer\n","\n","list_corpus = df_train[\"text\"].tolist()\n","list_labels = df_train[\"target\"].tolist()\n","\n","X_train, X_test, y_train, y_test = train_test_split(list_corpus, list_labels, test_size=0.2, \n","                                                                                random_state=40)\n","\n","X_train_counts, count_vectorizer = cv(X_train)\n","X_test_counts = count_vectorizer.transform(X_test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-07-06T21:53:17.856517Z","iopub.execute_input":"2021-07-06T21:53:17.856843Z","iopub.status.idle":"2021-07-06T21:53:18.417140Z","shell.execute_reply.started":"2021-07-06T21:53:17.856808Z","shell.execute_reply":"2021-07-06T21:53:18.415764Z"},"trusted":true,"id":"H5NgInwN1Tay"},"source":["\n","from sklearn.decomposition import PCA, TruncatedSVD\n","import matplotlib\n","import matplotlib.patches as mpatches\n","\n","\n","def plot_LSA(test_data, test_labels, savepath=\"PCA_demo.csv\", plot=True):\n","        lsa = TruncatedSVD(n_components=2)\n","        lsa.fit(test_data)\n","        lsa_scores = lsa.transform(test_data)\n","        color_mapper = {label:idx for idx,label in enumerate(set(test_labels))}\n","        color_column = [color_mapper[label] for label in test_labels]\n","        colors = ['orange','blue','blue']\n","        if plot:\n","            plt.scatter(lsa_scores[:,0], lsa_scores[:,1], s=8, alpha=.8, c=test_labels, cmap=matplotlib.colors.ListedColormap(colors))\n","            red_patch = mpatches.Patch(color='orange', label='Irrelevant')\n","            green_patch = mpatches.Patch(color='blue', label='Disaster')\n","            plt.legend(handles=[red_patch, green_patch], prop={'size': 30})\n","\n","\n","fig = plt.figure(figsize=(16, 16))          \n","plot_LSA(X_train_counts, y_train)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-07-06T21:53:18.419522Z","iopub.execute_input":"2021-07-06T21:53:18.420080Z","iopub.status.idle":"2021-07-06T21:53:21.051107Z","shell.execute_reply.started":"2021-07-06T21:53:18.420036Z","shell.execute_reply":"2021-07-06T21:53:21.049677Z"},"trusted":true,"id":"b1ybjzUI1Taz"},"source":["\n","from sklearn.linear_model import LogisticRegression\n","\n","clf = LogisticRegression(C=30.0, class_weight='balanced', solver='newton-cg', \n","                         multi_class='multinomial', n_jobs=-1, random_state=40)\n","clf.fit(X_train_counts, y_train)\n","\n","y_predicted_counts = clf.predict(X_test_counts)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-07-06T21:53:21.053083Z","iopub.execute_input":"2021-07-06T21:53:21.053707Z","iopub.status.idle":"2021-07-06T21:53:21.083021Z","shell.execute_reply.started":"2021-07-06T21:53:21.053658Z","shell.execute_reply":"2021-07-06T21:53:21.081444Z"},"trusted":true,"id":"E7itFML91Ta0"},"source":["#Evaluation to see if our classifier performed well at all\n","\n","from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report\n","\n","def get_metrics(y_test, y_predicted):  \n","    # true positives / (true positives+false positives)\n","    precision = precision_score(y_test, y_predicted, pos_label=None,\n","                                    average='weighted')             \n","    # true positives / (true positives + false negatives)\n","    recall = recall_score(y_test, y_predicted, pos_label=None,\n","                              average='weighted')\n","    \n","    # harmonic mean of precision and recall\n","    f1 = f1_score(y_test, y_predicted, pos_label=None, average='weighted')\n","    \n","    # true positives + true negatives/ total\n","    accuracy = accuracy_score(y_test, y_predicted)\n","    return accuracy, precision, recall, f1\n","\n","accuracy, precision, recall, f1 = get_metrics(y_test, y_predicted_counts)\n","print(\"accuracy = %.3f, precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracy, precision, recall, f1))\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-07-06T21:53:21.085141Z","iopub.execute_input":"2021-07-06T21:53:21.085520Z","iopub.status.idle":"2021-07-06T21:53:21.098426Z","shell.execute_reply.started":"2021-07-06T21:53:21.085484Z","shell.execute_reply":"2021-07-06T21:53:21.097093Z"},"trusted":true,"id":"o5vNoUIu1Ta0"},"source":["# Inspect the kind of mistakes our classifier is making by looking at the confusion matrix\n","\n","import numpy as np\n","import itertools\n","from sklearn.metrics import confusion_matrix\n","\n","def plot_confusion_matrix(cm, classes,\n","                          normalize=False,\n","                          title='Confusion matrix',\n","                          cmap=plt.cm.winter):\n","    if normalize:\n","        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n","    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n","    plt.title(title, fontsize=30)\n","    plt.colorbar()\n","    tick_marks = np.arange(len(classes))\n","    plt.xticks(tick_marks, classes, fontsize=20)\n","    plt.yticks(tick_marks, classes, fontsize=20)\n","    \n","    fmt = '.2f' if normalize else 'd'\n","    thresh = cm.max() / 2.\n","\n","    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n","        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", \n","                 color=\"white\" if cm[i, j] < thresh else \"black\", fontsize=40)\n","    \n","    plt.tight_layout()\n","    plt.ylabel('True label', fontsize=30)\n","    plt.xlabel('Predicted label', fontsize=30)\n","\n","    return plt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-07-06T21:53:21.101578Z","iopub.execute_input":"2021-07-06T21:53:21.101907Z","iopub.status.idle":"2021-07-06T21:53:21.431332Z","shell.execute_reply.started":"2021-07-06T21:53:21.101874Z","shell.execute_reply":"2021-07-06T21:53:21.430306Z"},"trusted":true,"id":"r6rX-TEB1Ta1"},"source":["cm = confusion_matrix(y_test, y_predicted_counts)\n","fig = plt.figure(figsize=(10, 10))\n","plot = plot_confusion_matrix(cm, classes=['Irrelevant','Disaster'], normalize=False, title='Confusion matrix')\n","plt.show()\n","print(cm)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-07-06T21:53:21.433319Z","iopub.execute_input":"2021-07-06T21:53:21.433726Z","iopub.status.idle":"2021-07-06T21:53:21.468693Z","shell.execute_reply.started":"2021-07-06T21:53:21.433685Z","shell.execute_reply":"2021-07-06T21:53:21.467134Z"},"trusted":true,"id":"OLjxVz6m1Ta1"},"source":["#Let's look at the features our classifier is using to make decisions.\n","\n","def get_most_important_features(vectorizer, model, n=5):\n","    index_to_word = {v:k for k,v in vectorizer.vocabulary_.items()}\n","    \n","    # loop for each class\n","    classes ={}\n","    for class_index in range(model.coef_.shape[0]):\n","        word_importances = [(el, index_to_word[i]) for i,el in enumerate(model.coef_[class_index])]\n","        sorted_coeff = sorted(word_importances, key = lambda x : x[0], reverse=True)\n","        tops = sorted(sorted_coeff[:n], key = lambda x : x[0])\n","        bottom = sorted_coeff[-n:]\n","        classes[class_index] = {\n","            'tops':tops,\n","            'bottom':bottom\n","        }\n","    return classes\n","\n","importance = get_most_important_features(count_vectorizer, clf, 10) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-07-06T21:53:21.470232Z","iopub.execute_input":"2021-07-06T21:53:21.470550Z","iopub.status.idle":"2021-07-06T21:53:21.845500Z","shell.execute_reply.started":"2021-07-06T21:53:21.470518Z","shell.execute_reply":"2021-07-06T21:53:21.844174Z"},"trusted":true,"id":"uVZb3oZx1Ta1"},"source":["#To see importance of words \n","\n","def plot_important_words(top_scores, top_words, bottom_scores, bottom_words, name):\n","    y_pos = np.arange(len(top_words))\n","    top_pairs = [(a,b) for a,b in zip(top_words, top_scores)]\n","    top_pairs = sorted(top_pairs, key=lambda x: x[1])\n","    \n","    bottom_pairs = [(a,b) for a,b in zip(bottom_words, bottom_scores)]\n","    bottom_pairs = sorted(bottom_pairs, key=lambda x: x[1], reverse=True)\n","    \n","    top_words = [a[0] for a in top_pairs]\n","    top_scores = [a[1] for a in top_pairs]\n","    \n","    bottom_words = [a[0] for a in bottom_pairs]\n","    bottom_scores = [a[1] for a in bottom_pairs]\n","    \n","    fig = plt.figure(figsize=(10, 10))  \n","\n","    plt.subplot(121)\n","    plt.barh(y_pos,bottom_scores, align='center', alpha=0.5)\n","    plt.title('Irrelevant', fontsize=20)\n","    plt.yticks(y_pos, bottom_words, fontsize=14)\n","    plt.suptitle('Key words', fontsize=16)\n","    plt.xlabel('Importance', fontsize=20)\n","    \n","    plt.subplot(122)\n","    plt.barh(y_pos,top_scores, align='center', alpha=0.5)\n","    plt.title('Disaster', fontsize=20)\n","    plt.yticks(y_pos, top_words, fontsize=14)\n","    plt.suptitle(name, fontsize=16)\n","    plt.xlabel('Importance', fontsize=20)\n","    \n","    plt.subplots_adjust(wspace=0.8)\n","    plt.show()\n","\n","top_scores = [a[0] for a in importance[0]['tops']]\n","top_words = [a[1] for a in importance[0]['tops']]\n","bottom_scores = [a[0] for a in importance[0]['bottom']]\n","bottom_words = [a[1] for a in importance[0]['bottom']]\n","\n","plot_important_words(top_scores, top_words, bottom_scores, bottom_words, \"Most important words for relevance\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-07-06T21:53:21.847497Z","iopub.execute_input":"2021-07-06T21:53:21.847861Z","iopub.status.idle":"2021-07-06T21:53:22.123504Z","shell.execute_reply.started":"2021-07-06T21:53:21.847825Z","shell.execute_reply":"2021-07-06T21:53:22.122082Z"},"trusted":true,"id":"1e18GNkZ1Ta2"},"source":["# TF-IDF (Term Frequency, Inverse Document Frequency) :discounting words that are too frequent, as they just add to the noise.\n","\n","def tfidf(data):\n","    tfidf_vectorizer = TfidfVectorizer()\n","\n","    train = tfidf_vectorizer.fit_transform(data)\n","\n","    return train, tfidf_vectorizer\n","\n","X_train_tfidf, tfidf_vectorizer = tfidf(X_train)\n","X_test_tfidf = tfidf_vectorizer.transform(X_test)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-07-06T21:53:22.125533Z","iopub.execute_input":"2021-07-06T21:53:22.126200Z","iopub.status.idle":"2021-07-06T21:53:22.130941Z","shell.execute_reply.started":"2021-07-06T21:53:22.126150Z","shell.execute_reply":"2021-07-06T21:53:22.129305Z"},"trusted":true,"id":"l8IopzvN1Ta3"},"source":["#fig = plt.figure(figsize=(16, 16))          \n","#plot_LSA(X_train_tfidf, y_train)\n","#plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-07-06T21:53:22.133430Z","iopub.execute_input":"2021-07-06T21:53:22.133907Z","iopub.status.idle":"2021-07-06T21:53:23.226536Z","shell.execute_reply.started":"2021-07-06T21:53:22.133861Z","shell.execute_reply":"2021-07-06T21:53:23.224767Z"},"trusted":true,"id":"4FILeUb21Ta3"},"source":["#Evaluation to see if our classifier performed well at all\n","clf_tfidf = LogisticRegression(C=30.0, class_weight='balanced', solver='newton-cg', \n","                         multi_class='multinomial', n_jobs=-1, random_state=40)\n","clf_tfidf.fit(X_train_tfidf, y_train)\n","\n","y_predicted_tfidf = clf_tfidf.predict(X_test_tfidf)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-07-06T21:53:23.228534Z","iopub.execute_input":"2021-07-06T21:53:23.229158Z","iopub.status.idle":"2021-07-06T21:53:23.250236Z","shell.execute_reply.started":"2021-07-06T21:53:23.229091Z","shell.execute_reply":"2021-07-06T21:53:23.248928Z"},"trusted":true,"id":"w_LkGSjj1Ta3"},"source":["accuracy_tfidf, precision_tfidf, recall_tfidf, f1_tfidf = get_metrics(y_test, y_predicted_tfidf)\n","print(\"accuracy = %.3f, precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracy_tfidf, precision_tfidf, \n","                                                                       recall_tfidf, f1_tfidf))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-07-06T21:53:23.251725Z","iopub.execute_input":"2021-07-06T21:53:23.252245Z","iopub.status.idle":"2021-07-06T21:53:23.508358Z","shell.execute_reply.started":"2021-07-06T21:53:23.252202Z","shell.execute_reply":"2021-07-06T21:53:23.507139Z"},"trusted":true,"id":"YfnrYeDo1Ta3"},"source":["#Confusion matrix\n","\n","cm2 = confusion_matrix(y_test, y_predicted_tfidf)\n","fig = plt.figure(figsize=(10, 10))\n","plot = plot_confusion_matrix(cm2, classes=['Irrelevant','Disaster'], normalize=False, title='Confusion matrix')\n","plt.show()\n","print(\"TFIDF confusion matrix\")\n","print(cm2)\n","print(\"BoW confusion matrix\")\n","print(cm)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-07-06T21:53:23.510032Z","iopub.execute_input":"2021-07-06T21:53:23.510371Z","iopub.status.idle":"2021-07-06T21:53:23.898480Z","shell.execute_reply.started":"2021-07-06T21:53:23.510335Z","shell.execute_reply":"2021-07-06T21:53:23.897329Z"},"trusted":true,"id":"5gLyB1JO1Ta4"},"source":["#Looking at important coefficients for linear regression\n","\n","importance_tfidf = get_most_important_features(tfidf_vectorizer, clf_tfidf, 10)\n","\n","top_scores = [a[0] for a in importance_tfidf[0]['tops']]\n","top_words = [a[1] for a in importance_tfidf[0]['tops']]\n","bottom_scores = [a[0] for a in importance_tfidf[0]['bottom']]\n","bottom_words = [a[1] for a in importance_tfidf[0]['bottom']]\n","\n","plot_important_words(top_scores, top_words, bottom_scores, bottom_words, \"Most important words for relevance\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZiDZL1xjAI8f"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"execution":{"iopub.status.busy":"2021-07-06T20:48:19.146185Z","iopub.execute_input":"2021-07-06T20:48:19.146594Z","iopub.status.idle":"2021-07-06T20:48:19.184056Z","shell.execute_reply.started":"2021-07-06T20:48:19.14655Z","shell.execute_reply":"2021-07-06T20:48:19.182026Z"},"id":"FL2qYMmu1Ta4"},"source":["\n"]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-07-06T21:53:23.900164Z","iopub.execute_input":"2021-07-06T21:53:23.900806Z","iopub.status.idle":"2021-07-06T21:54:21.309637Z","shell.execute_reply.started":"2021-07-06T21:53:23.900753Z","shell.execute_reply":"2021-07-06T21:54:21.308699Z"},"trusted":true,"id":"6u68LbEg1Ta4"},"source":["import gzip\n","\n","import gensim\n","import gensim.downloader as api\n","from gensim.models import KeyedVectors\n","\n","#filename = '../input/googles-trained-word2vec-model-in-python/GoogleNews-vectors-negative300.bin'\n","#model = KeyedVectors.load_word2vec_format(filename, binary=True)\n","\n","# Load Google's pre-trained Word2Vec model.\n","#rd2vec_path=gzip.open('/content/GoogleNews-vectors-negative300.bin.zip', 'rt')\n","word2vec_path = '/content/GoogleNews-vectors-negative300.bin.zip'\n","\n","word2vec = gensim.models.KeyedVectors.load_word2vec_format(word2vec_path, binary=True)\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JssPQKJjCi0_"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-07-06T21:54:21.310896Z","iopub.execute_input":"2021-07-06T21:54:21.311381Z","iopub.status.idle":"2021-07-06T21:54:21.320679Z","shell.execute_reply.started":"2021-07-06T21:54:21.311347Z","shell.execute_reply":"2021-07-06T21:54:21.319470Z"},"trusted":true,"id":"F4YWkvOz1Ta4"},"source":["\n","\n","def get_average_word2vec(tokens_list, vector, generate_missing=False, k=300):\n","    if len(tokens_list)<1:\n","        return np.zeros(k)\n","    if generate_missing:\n","        vectorized = [vector[word] if word in vector else np.random.rand(k) for word in tokens_list]\n","    else:\n","        vectorized = [vector[word] if word in vector else np.zeros(k) for word in tokens_list]\n","    length = len(vectorized)\n","    summed = np.sum(vectorized, axis=0)\n","    averaged = np.divide(summed, length)\n","    return averaged\n","\n","def get_word2vec_embeddings(vectors, df_train, generate_missing=False):\n","    embeddings = df_train['tokens'].apply(lambda x: get_average_word2vec(x, vectors, \n","                                                                                generate_missing=generate_missing))\n","    return list(embeddings)\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-07-06T21:54:21.325366Z","iopub.execute_input":"2021-07-06T21:54:21.325764Z","iopub.status.idle":"2021-07-06T21:54:21.967700Z","shell.execute_reply.started":"2021-07-06T21:54:21.325710Z","shell.execute_reply":"2021-07-06T21:54:21.966440Z"},"trusted":true,"id":"IisWY3Wy1Ta4"},"source":["embeddings = get_word2vec_embeddings(word2vec, df_train)\n","X_train_word2vec, X_test_word2vec, y_train_word2vec, y_test_word2vec = train_test_split(embeddings, list_labels, \n","                                                                                        test_size=0.2, random_state=40)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-07-06T21:54:21.969731Z","iopub.execute_input":"2021-07-06T21:54:21.970148Z","iopub.status.idle":"2021-07-06T21:54:22.932243Z","shell.execute_reply.started":"2021-07-06T21:54:21.970113Z","shell.execute_reply":"2021-07-06T21:54:22.931342Z"},"trusted":true,"id":"y5HNkUgs1Ta5"},"source":["fig = plt.figure(figsize=(16, 16))          \n","plot_LSA(embeddings, list_labels)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-07-06T21:54:22.933457Z","iopub.execute_input":"2021-07-06T21:54:22.933921Z","iopub.status.idle":"2021-07-06T21:54:23.778494Z","shell.execute_reply.started":"2021-07-06T21:54:22.933869Z","shell.execute_reply":"2021-07-06T21:54:23.777176Z"},"trusted":true,"id":"QlK0wDsn1Ta5"},"source":["#Logistic Regression\n","\n","clf_w2v = LogisticRegression(C=30.0, class_weight='balanced', solver='newton-cg', \n","                         multi_class='multinomial', random_state=40)\n","clf_w2v.fit(X_train_word2vec, y_train_word2vec)\n","y_predicted_word2vec = clf_w2v.predict(X_test_word2vec)\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-07-06T21:54:23.780271Z","iopub.execute_input":"2021-07-06T21:54:23.780940Z","iopub.status.idle":"2021-07-06T21:54:23.810161Z","shell.execute_reply.started":"2021-07-06T21:54:23.780881Z","shell.execute_reply":"2021-07-06T21:54:23.809006Z"},"trusted":true,"id":"Zns818BE1Ta5"},"source":["accuracy_word2vec, precision_word2vec, recall_word2vec, f1_word2vec = get_metrics(y_test_word2vec, y_predicted_word2vec)\n","print(\"accuracy = %.3f, precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracy_word2vec, precision_word2vec, \n","                                                                       recall_word2vec, f1_word2vec))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-07-06T21:54:23.811998Z","iopub.execute_input":"2021-07-06T21:54:23.812746Z","iopub.status.idle":"2021-07-06T21:54:24.168002Z","shell.execute_reply.started":"2021-07-06T21:54:23.812689Z","shell.execute_reply":"2021-07-06T21:54:24.166461Z"},"trusted":true,"id":"gCdb2U_z1Ta5"},"source":["#Confusion matrix\n","\n","\n","cm_w2v = confusion_matrix(y_test_word2vec, y_predicted_word2vec)\n","fig = plt.figure(figsize=(10, 10))\n","plot = plot_confusion_matrix(cm, classes=['Irrelevant','Disaster'], normalize=False, title='Confusion matrix')\n","plt.show()\n","print(\"Word2Vec confusion matrix\")\n","print(cm_w2v)\n","print(\"TFIDF confusion matrix\")\n","print(cm2)\n","print(\"BoW confusion matrix\")\n","print(cm)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-07-06T21:54:24.169669Z","iopub.execute_input":"2021-07-06T21:54:24.170061Z","iopub.status.idle":"2021-07-06T21:54:24.683121Z","shell.execute_reply.started":"2021-07-06T21:54:24.170024Z","shell.execute_reply":"2021-07-06T21:54:24.681835Z"},"trusted":true,"id":"tRCC_7rv1Ta6"},"source":["#Lime\n","\n","from lime import lime_text\n","from sklearn.pipeline import make_pipeline\n","from lime.lime_text import LimeTextExplainer\n","\n","X_train_data, X_test_data, y_train_data, y_test_data = train_test_split(list_corpus, list_labels, test_size=0.2, \n","                                                                                random_state=40)\n","vector_store = word2vec\n","def word2vec_pipeline(examples):\n","    global vector_store\n","    tokenizer = RegexpTokenizer(r'\\w+')\n","    tokenized_list = []\n","    for example in examples:\n","        example_tokens = tokenizer.tokenize(example)\n","        vectorized_example = get_average_word2vec(example_tokens, vector_store, generate_missing=False, k=300)\n","        tokenized_list.append(vectorized_example)\n","    return clf_w2v.predict_proba(tokenized_list)\n","\n","c = make_pipeline(count_vectorizer, clf)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-07-06T21:54:24.684559Z","iopub.execute_input":"2021-07-06T21:54:24.684864Z","iopub.status.idle":"2021-07-06T21:54:24.692480Z","shell.execute_reply.started":"2021-07-06T21:54:24.684833Z","shell.execute_reply":"2021-07-06T21:54:24.691113Z"},"trusted":true,"id":"E3w8GqwT1Ta6"},"source":["def explain_one_instance(instance, class_names):\n","    explainer = LimeTextExplainer(class_names=class_names)\n","    exp = explainer.explain_instance(instance, word2vec_pipeline, num_features=6)\n","    return exp\n","\n","def visualize_one_exp(features, labels, index, class_names = [\"irrelevant\",\"relevant\", \"unknown\"]):\n","    exp = explain_one_instance(features[index], class_names = class_names)\n","    print('Index: %d' % index)\n","    print('True class: %s' % class_names[labels[index]])\n","    exp.show_in_notebook(text=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-07-06T21:54:24.694322Z","iopub.execute_input":"2021-07-06T21:54:24.694801Z","iopub.status.idle":"2021-07-06T21:54:24.705597Z","shell.execute_reply.started":"2021-07-06T21:54:24.694757Z","shell.execute_reply":"2021-07-06T21:54:24.704491Z"},"trusted":true,"id":"WJoUmrHA1Ta6"},"source":["#visualize_one_exp(X_test_data, y_test_data, 65)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-07-06T21:54:24.707359Z","iopub.execute_input":"2021-07-06T21:54:24.707734Z","iopub.status.idle":"2021-07-06T21:54:24.722560Z","shell.execute_reply.started":"2021-07-06T21:54:24.707697Z","shell.execute_reply":"2021-07-06T21:54:24.721361Z"},"trusted":true,"id":"vIe7bdnK1Ta6"},"source":["#visualize_one_exp(X_test_data, y_test_data, 60)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-07-06T21:54:24.724271Z","iopub.execute_input":"2021-07-06T21:54:24.724633Z","iopub.status.idle":"2021-07-06T21:54:50.977846Z","shell.execute_reply.started":"2021-07-06T21:54:24.724595Z","shell.execute_reply":"2021-07-06T21:54:50.973138Z"},"trusted":true,"id":"nsCcd2Mn1Ta6"},"source":["\n","import random\n","from collections import defaultdict\n","\n","random.seed(40)\n","\n","def get_statistical_explanation(test_set, sample_size, word2vec_pipeline, label_dict):\n","    sample_sentences = random.sample(test_set, sample_size)\n","    explainer = LimeTextExplainer()\n","    \n","    labels_to_sentences = defaultdict(list)\n","    contributors = defaultdict(dict)\n","    \n","    # First, find contributing words to each class\n","    for sentence in sample_sentences:\n","        probabilities = word2vec_pipeline([sentence])\n","        curr_label = probabilities[0].argmax()\n","        labels_to_sentences[curr_label].append(sentence)\n","        exp = explainer.explain_instance(sentence, word2vec_pipeline, num_features=6, labels=[curr_label])\n","        listed_explanation = exp.as_list(label=curr_label)\n","        \n","        for word,contributing_weight in listed_explanation:\n","            if word in contributors[curr_label]:\n","                contributors[curr_label][word].append(contributing_weight)\n","            else:\n","                contributors[curr_label][word] = [contributing_weight]    \n","    \n","    # average each word's contribution to a class, and sort them by impact\n","    average_contributions = {}\n","    sorted_contributions = {}\n","    for label,lexica in contributors.items():\n","        curr_label = label\n","        curr_lexica = lexica\n","        average_contributions[curr_label] = pd.Series(index=curr_lexica.keys())\n","        for word,scores in curr_lexica.items():\n","            average_contributions[curr_label].loc[word] = np.sum(np.array(scores))/sample_size\n","        detractors = average_contributions[curr_label].sort_values()\n","        supporters = average_contributions[curr_label].sort_values(ascending=False)\n","        sorted_contributions[label_dict[curr_label]] = {\n","            'detractors':detractors,\n","             'supporters': supporters\n","        }\n","    return sorted_contributions\n","\n","label_to_text = {\n","    0: 'Irrelevant',\n","    1: 'Relevant',\n","    2: 'Unsure'\n","}\n","sorted_contributions = get_statistical_explanation(X_test_data, 100, word2vec_pipeline, label_to_text)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-07-06T21:54:50.979229Z","iopub.status.idle":"2021-07-06T21:54:50.979839Z"},"trusted":true,"id":"X3AtNUBh1Ta7"},"source":["\n","# First index is the class (Disaster)\n","# Second index is 0 for detractors, 1 for supporters\n","# Third is how many words we sample\n","top_words = sorted_contributions['Relevant']['supporters'][:10].index.tolist()\n","top_scores = sorted_contributions['Relevant']['supporters'][:10].tolist()\n","bottom_words = sorted_contributions['Relevant']['detractors'][:10].index.tolist()\n","bottom_scores = sorted_contributions['Relevant']['detractors'][:10].tolist()\n","\n","plot_important_words(top_scores, top_words, bottom_scores, bottom_words, \"Most important words for relevance\")"],"execution_count":null,"outputs":[]}]}